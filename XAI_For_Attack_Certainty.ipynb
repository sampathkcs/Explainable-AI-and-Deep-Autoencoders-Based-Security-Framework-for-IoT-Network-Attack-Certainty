{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f03eeb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports various libraries including NumPy, pandas, TensorFlow, and SHAP. \n",
    "#These libraries are used for numerical operations, data manipulation, machine learning, and SHAP explanations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logger = logging.getLogger('shap')\n",
    "logger.disabled = True\n",
    "\n",
    "class AnomalyExplanationUsingSHAP:\n",
    "    '''\n",
    "    This class implements the method described in 'Explaining Anomalies Detected by Autoencoders Using SHAP' to explain\n",
    "    anomalies revealed by an unsupervised Autoencoder model using SHAP.\n",
    "    '''\n",
    "    \n",
    "    model_autoencoder = None\n",
    "    num_anomalies_to_explain = None\n",
    "    reconstruction_error_percentage = None\n",
    "    shap_values_selection_method = None\n",
    "    counter = None\n",
    "\n",
    "    def __init__(self, num_anomalies=100, reconstruction_error_percent=0.5, shap_selection='mean'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_anomalies_to_explain (int): number of top ranked anomalies (ranked by anomaly score that is the mse) to\n",
    "                                            explain.\n",
    "            reconstruction_error_percent (float): Number between 0 to 1- see explanation to this parameter in\n",
    "                                                  'Explaining Anomalies Detected by Autoencoders Using SHAP' under\n",
    "                                                  ReconstructionErrorPercent.\n",
    "            shap_values_selection (str): One of the possible methods to choose explaining features by their SHAP values.\n",
    "                                         Can be: 'mean', 'median', 'constant'. See explanation to this parameter in\n",
    "                                         'Explaining Anomalies Detected by Autoencoders Using SHAP' under\n",
    "                                         SHAPvaluesSelection.\n",
    "        \"\"\"\n",
    "        self.num_anomalies_to_explain = num_anomalies\n",
    "        self.reconstruction_error_percentage = reconstruction_error_percent\n",
    "        self.shap_values_selection_method = shap_selection\n",
    "\n",
    "    def train_autoencoder(self, train_data, epochs=5, batch_size=64):\n",
    "        \"\"\"\n",
    "        Train 6-layer Autoencoder model on the given x_train data.\n",
    "\n",
    "        Args:\n",
    "            x_train (data frame): The data to train the Autoencoder model on\n",
    "            nb_epoch (int): Number of epoch the model will perform\n",
    "            batch_size (int): Size of each batch of data enter to the model\n",
    "\n",
    "        Returns:\n",
    "            model: Trained autoencoder\n",
    "        \"\"\"\n",
    "        input_dim = train_data.shape[1]\n",
    "        print('Training data features:', input_dim)\n",
    "\n",
    "        input_layer = Input(shape=(input_dim,))\n",
    "        encoder = Dense(10, activation=\"relu\", activity_regularizer=regularizers.l1(10e-7))(input_layer)\n",
    "        encoder = Dense(32, activation=\"relu\", kernel_regularizer=regularizers.l2(10e-7))(encoder)\n",
    "        decoder = Dense(10, activation='relu', kernel_regularizer=regularizers.l2(10e-7))(encoder)\n",
    "        decoder = Dense(32, activation='relu', kernel_regularizer=regularizers.l2(10e-7))(decoder)\n",
    "        decoder = Dense(input_dim, activation=None, kernel_regularizer=regularizers.l2(10e-7))(decoder)\n",
    "\n",
    "        self.model_autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "        self.model_autoencoder.summary()\n",
    "\n",
    "        self.model_autoencoder.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
    "\n",
    "        early_stopper = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "        self.model_autoencoder.fit(train_data, train_data, epochs=epochs, batch_size=batch_size, shuffle=True,\n",
    "                                   validation_split=0.1, verbose=2, callbacks=[early_stopper])\n",
    "        self.model_autoencoder.save(\"autoencoder.h5\")\n",
    "\n",
    "        return self.model_autoencoder\n",
    "                \n",
    "    def get_top_anomalies_to_explain(self, explain_data):\n",
    "        \"\"\"\n",
    "        Sort all records in x_explain by their MSE calculated according to their prediction by the trained Autoencoder\n",
    "        and return the top num_anomalies_to_explain (its value given by the user at class initialization) records.\n",
    "\n",
    "        Args:\n",
    "            x_explain (data frame): Set of records we want to explain the most anomalous ones from it.\n",
    "\n",
    "        Returns:\n",
    "            list: List of index of the top num_anomalies_to_explain records with highest MSE that will be explained.\n",
    "        \"\"\"\n",
    "        predictions = self.model_autoencoder.predict(explain_data)\n",
    "        squared_errors = np.power(explain_data - predictions, 2)\n",
    "        mse_series = pd.Series(np.mean(squared_errors, axis=1))\n",
    "\n",
    "        most_anomalous_transactions = mse_series.sort_values(ascending=False)\n",
    "        columns = [\"id\", \"mse_all_columns\"]\n",
    "        columns.extend([\"squared_error_\" + col for col in list(explain_data.columns)])\n",
    "        items = []\n",
    "        for x in most_anomalous_transactions.iteritems():\n",
    "            item = [x[0], x[1]]\n",
    "            item.extend(squared_errors.loc[x[0]])\n",
    "            items.append(item)\n",
    "\n",
    "        anomalies_df = pd.DataFrame(items, columns=columns)\n",
    "        anomalies_df.set_index('id', inplace=True)\n",
    "\n",
    "        top_anomalies_to_explain = anomalies_df.head(self.num_anomalies_to_explain).index\n",
    "        return top_anomalies_to_explain\n",
    "\n",
    "    def get_num_features_with_high_reconstruction_error(self, total_squared_error, errors_df):\n",
    "        \"\"\"\n",
    "        Calculate the number of features whose reconstruction errors sum to reconstruction_error_percent of the\n",
    "        total_squared_error of the records that selected to be explained at the moment. This is the number of the\n",
    "        top reconstructed errors features that going to be explained and eventually this features together with their\n",
    "        explanation will build up the features explanation set of this record.\n",
    "\n",
    "        Args:\n",
    "            total_squared_error (int): MSE of the records selected to be explained\n",
    "            errors_df (data frame): The reconstruction error of each feature- this is the first output output of\n",
    "                                    get_errors_df_per_record function\n",
    "\n",
    "        Returns:\n",
    "            int: Number of features whose reconstruction errors sum to reconstruction_error_percent of the\n",
    "                 total_squared_error of the records that selected to be explained at the moment\n",
    "        \"\"\"\n",
    "        error_sum = 0\n",
    "        num_features = 0\n",
    "        for index in errors_df.index:\n",
    "            error_sum += errors_df.loc[index, 'err']\n",
    "            if error_sum >= self.reconstruction_error_percentage * total_squared_error:\n",
    "                break\n",
    "            num_features += 1\n",
    "        return num_features\n",
    "\n",
    "    def get_background_set(self, train_data, background_size=200):\n",
    "        \"\"\"\n",
    "        Get the first background_size records from x_train data and return it. Used for SHAP explanation process.\n",
    "\n",
    "        Args:\n",
    "            x_train (data frame): the data we will get the background set from\n",
    "            background_size (int): The number of records to select from x_train. Default value is 200.\n",
    "\n",
    "        Returns:\n",
    "            data frame: Records from x_train that will be the background set of the explanation of the record that we\n",
    "                        explain at that moment using SHAP.\n",
    "        \"\"\"\n",
    "        background_set = train_data.head(background_size)\n",
    "        return background_set\n",
    "\n",
    "    def get_error_df_per_record(self, record):\n",
    "        \"\"\"\n",
    "        Create data frame of the reconstruction errors of each features of the given record. Eventually we get data\n",
    "        frame so each row contain the index of feature, its name, and its reconstruction error based on the record\n",
    "        prediction provided by the trained autoencoder. This data frame is sorted by the reconstruction error of the\n",
    "        features\n",
    "\n",
    "        Args:\n",
    "            record (pandas series): The record we explain at the moment; values of all its features.\n",
    "\n",
    "        Returns:\n",
    "            data frame: Data frame of all features reconstruction error sorted by the reconstruction error.\n",
    "        \"\"\"\n",
    "        prediction = self.model_autoencoder.predict(np.array([[record]])[0])[0]\n",
    "        squared_errors = np.power(record - prediction, 2)\n",
    "        errors_df = pd.DataFrame({'col_name': squared_errors.index, 'err': squared_errors}).reset_index(drop=True)\n",
    "        total_mse = np.mean(squared_errors)\n",
    "        errors_df.sort_values(by='err', ascending=False, inplace=True)\n",
    "        return errors_df, total_mse\n",
    "\n",
    "    def get_high_shap_values(self, shap_values_df):\n",
    "        \"\"\"\n",
    "        Choosing explaining features based on their SHAP values by shap_values_selection method (mean, median, constant)\n",
    "        i.e. remove all features with SHAP values that do not meet the method requirements as described in 'Explaining\n",
    "        Anomalies Detected by Autoencoders Using SHAP' under SHAPvaluesSelection.\n",
    "\n",
    "        Args:\n",
    "            shap_values_df (data frame): Data frame with all existing features and their SHAP values.\n",
    "\n",
    "        Returns:\n",
    "            data frame: Data frame that contain for each feature we explain (features with high reconstruction error)\n",
    "                        its explaining features that selected by the shap_values_selection method and their SHAP values.\n",
    "        \"\"\"\n",
    "        high_contributing_features_df = pd.DataFrame()\n",
    "\n",
    "        for i in range(shap_values_df.shape[0]):\n",
    "            shap_values = shap_values_df.iloc[i]\n",
    "            if self.shap_values_selection_method == 'mean':\n",
    "                threshold_value = np.mean(shap_values)\n",
    "            elif self.shap_values_selection_method == 'median':\n",
    "                threshold_value = np.median(shap_values)\n",
    "            elif self.shap_values_selection_method == 'constant':\n",
    "                num_explaining_features = 5\n",
    "                explaining_features = shap_values_df[i:i + 1].stack().nlargest(num_explaining_features)\n",
    "                high_contributing_features_df = pd.concat([high_contributing_features_df, explaining_features], axis=0)\n",
    "                continue\n",
    "            else:\n",
    "                raise ValueError('Unknown SHAP value selection method')\n",
    "\n",
    "            num_contributing_features = 0\n",
    "            for j in range(len(shap_values)):\n",
    "                if shap_values[j] > threshold_value:\n",
    "                    num_contributing_features += 1\n",
    "            contributing_features = shap_values_df[i:i + 1].stack().nlargest(num_contributing_features)\n",
    "            high_contributing_features_df = pd.concat([high_contributing_features_df, contributing_features], axis=0)\n",
    "        return high_contributing_features_df\n",
    "\n",
    "    def predict_feature(self, record):\n",
    "        \"\"\"\n",
    "        Predict the value of specific feature (with 'counter' index) using the trained autoencoder\n",
    "\n",
    "        Args:\n",
    "            record (pandas series): The record we explain at the moment; values of all its features.\n",
    "\n",
    "        Returns:\n",
    "            list: List the size of the number of features, contain the value of the predicted features with 'counter'\n",
    "                  index (the feature we explain at the moment)\n",
    "        \"\"\"\n",
    "\n",
    "        record_prediction = self.model_autoencoder.predict(record)[:, self.counter]\n",
    "        return record_prediction\n",
    "\n",
    "    def explain_unsupervised_data(self, train_data, explain_data, autoencoder=None, return_shap_values=False):\n",
    "        \"\"\"\n",
    "        Explanation Process:\n",
    "        1. If no Autoencoder model is provided ('autoencoder' is None), a new Autoencoder model is trained on the given 'x_train' data.\n",
    "        2. For each record in 'top_records_to_explain', selected from the given 'x_explain' based on the 'get_top_anomaly_to_explain' function:\n",
    "           a. SHAP is used to explain features with the highest reconstruction error as determined by the 'get_num_features_with_highest_reconstruction_error' function.\n",
    "           b. SHAP values of each feature in the explanation are obtained for the high-reconstruction-error feature.\n",
    "           c. Explaining features are selected using the 'highest_contributing_features' function.\n",
    "        3. The explanation features set is constructed:\n",
    "           a. Starting with the feature with the highest reconstruction error and its explaining features.\n",
    "           b. Subsequent features with lower reconstruction errors and their corresponding explaining features are added, ensuring no duplicates.\n",
    "\n",
    "        Args:\n",
    "            x_train (DataFrame): Data used to train the autoencoder model and select the background set for SHAP explanation.\n",
    "            x_explain (DataFrame): Data from which the top 'num_anomalies_to_explain' records are selected based on their MSE for explanation.\n",
    "            autoencoder (model): Trained Autoencoder model to explain 'x_explain' data. If None, a new Autoencoder model is trained as described in the 'train_model' function.\n",
    "            return_shap_values (bool): If False, explanation feature sets for each record include only feature names. If True, SHAP values are included, forming tuples of (str, float) where str is the explaining feature name and float is its SHAP value.\n",
    "                                      Note: Explained features with no previous feature explanation (higher reconstruction error) will have a unique SHAP value of -1.\n",
    "        Returns:\n",
    "            dict: A dictionary containing explanations for 'top_records_to_explain' records. Keys are record indexes (int), and values are lists representing explanation feature sets.\n",
    "        \"\"\"\n",
    "\n",
    "        self.model_autoencoder = autoencoder\n",
    "        if self.model_autoencoder is None:\n",
    "            self.train_autoencoder(train_data)\n",
    "\n",
    "        top_anomalies_to_explain = self.get_top_anomalies_to_explain(explain_data)\n",
    "        all_feature_sets_explanation = {}\n",
    "\n",
    "        for record_idx in top_anomalies_to_explain:\n",
    "            record_to_explain = explain_data.loc[record_idx]\n",
    "            error_df, total_mse = self.get_error_df_per_record(record_to_explain)\n",
    "            num_of_features = self.get_num_features_with_high_reconstruction_error(total_mse * error_df.shape[0], error_df)\n",
    "\n",
    "            top_error_df = error_df.head(num_of_features)\n",
    "            all_feature_sets_explanation[record_idx] = []\n",
    "            shap_values_all_features = [[] for num in range(num_of_features)]\n",
    "\n",
    "            background_set = self.get_background_set(train_data, 200).values\n",
    "            for i in range(num_of_features):\n",
    "                self.counter = top_error_df.index[i]\n",
    "                explainer = shap.KernelExplainer(self.predict_feature, background_set)\n",
    "                shap_values = explainer.shap_values(record_to_explain, nsamples='auto')\n",
    "                shap_values_all_features[i] = shap_values\n",
    "\n",
    "            shap_values_all_features = np.fabs(shap_values_all_features)\n",
    "            shap_values_all_features = pd.DataFrame(data=shap_values_all_features, columns=train_data.columns)\n",
    "            highest_contributing_features = self.get_high_shap_values(shap_values_all_features)\n",
    "\n",
    "            for idx_explained_feature in range(num_of_features):\n",
    "                feature_set = []\n",
    "                for idx, row in highest_contributing_features.iterrows():\n",
    "                    if idx[0] == idx_explained_feature:\n",
    "                        feature_set.append((idx[1], row[0]))\n",
    "                explained_feature_index = top_error_df.index[idx_explained_feature]\n",
    "                feature_set.insert(0, (train_data.columns[explained_feature_index], -1))\n",
    "\n",
    "                all_feature_sets_explanation[record_idx].append(feature_set)\n",
    "\n",
    "            final_feature_set = []\n",
    "            final_feature_items = []\n",
    "            for item in sum(all_feature_sets_explanation[record_idx], []):\n",
    "                if item[0] not in final_feature_set:\n",
    "                    final_feature_set.append(item[0])\n",
    "                    final_feature_items.append(item)\n",
    "\n",
    "            if return_shap_values:\n",
    "                all_feature_sets_explanation[record_idx] = final_feature_items\n",
    "            else:\n",
    "                all_feature_sets_explanation[record_idx] = final_feature_set\n",
    "\n",
    "        return all_feature_sets_explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc786c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d198c5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the benign dataset from the specified CSV file\n",
    "benign = pd.read_csv(r\"C:/Users\\sampa\\Desktop\\dataset\\REGULAR.csv\")\n",
    "# Read the attack dataset from the specified CSV file\n",
    "Attack = pd.read_csv(r\"C:/Users\\sampa\\Desktop\\dataset\\IOT\\Hulk-Evasive.csv\")\n",
    "\n",
    "\n",
    "# selected columns (Add more columns)\n",
    "# cols is the column list for benign dataset\n",
    "cols = ['Flow Packets/s', 'Fwd PSH Flags', 'Fwd Packets/s', 'Bwd Packets/s','FIN Flag Count', 'PSH Flag Count', 'Fwd Bytes/Bulk Avg',\n",
    "       'Fwd Packet/Bulk Avg', 'Fwd Bulk Rate Avg', 'Fwd Act Data Pkts','Fwd Packet Length Max', 'Fwd Packet Length Mean',\n",
    "       'Fwd Packet Length Std', 'Bwd Packet Length Max','Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Packet Length Max',\n",
    "       'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance','RST Flag Count', 'Average Packet Size', 'Fwd Segment Size Avg',\n",
    "       'Bwd Segment Size Avg', 'Subflow Fwd Bytes', 'Subflow Bwd Bytes']\n",
    "\n",
    "# catt_cols is the column list for attack dataset (including Label column)\n",
    "att_cols = ['Flow Packets/s', 'Fwd PSH Flags', 'Fwd Packets/s', 'Bwd Packets/s','FIN Flag Count', 'PSH Flag Count', 'Fwd Bytes/Bulk Avg',\n",
    "       'Fwd Packet/Bulk Avg', 'Fwd Bulk Rate Avg', 'Fwd Act Data Pkts','Fwd Packet Length Max', 'Fwd Packet Length Mean',\n",
    "       'Fwd Packet Length Std', 'Bwd Packet Length Max','Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Packet Length Max',\n",
    "       'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance','RST Flag Count', 'Average Packet Size', 'Fwd Segment Size Avg',\n",
    "       'Bwd Segment Size Avg', 'Subflow Fwd Bytes', 'Subflow Bwd Bytes','Label']\n",
    "\n",
    "#%%\n",
    "# filter columns\n",
    "benign = benign[cols]\n",
    "# remove inf values\n",
    "benign = benign.replace([np.inf], np.nan)\n",
    "benign.dropna(inplace=True)\n",
    "\n",
    "# filter columns\n",
    "Attack = Attack[att_cols]\n",
    "# remove inf values\n",
    "Attack = Attack.replace([np.inf], np.nan)\n",
    "Attack.dropna(inplace=True)\n",
    "\n",
    "#%%\n",
    "# data normalization (this will convert all columns to 0-1 range)\n",
    "scaler = MinMaxScaler()\n",
    "fit_scaling = scaler.fit(benign)\n",
    "fit_apply = fit_scaling.transform(benign)  # fit the scaler for benign dataset\n",
    "benign_norm = pd.DataFrame(fit_apply, columns=benign.columns, index=benign.index)\n",
    "\n",
    "Attack_label = Attack[['Label']]\n",
    "Attack_X = Attack.drop('Label', axis=1)\n",
    "fit_apply = fit_scaling.transform(Attack_X)  # apply normalization to attack dataset\n",
    "attack_norm = pd.DataFrame(fit_apply, columns=Attack_X.columns, index=Attack_X.index)\n",
    "\n",
    "\n",
    "\n",
    "# data preparation for autoencoder model\n",
    "X_train = benign_norm   # convert pandas dataframe into a array\n",
    "X_test = attack_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b72324f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Flow Packets/s</th>\n",
       "      <th>Fwd PSH Flags</th>\n",
       "      <th>Fwd Packets/s</th>\n",
       "      <th>Bwd Packets/s</th>\n",
       "      <th>FIN Flag Count</th>\n",
       "      <th>PSH Flag Count</th>\n",
       "      <th>Fwd Bytes/Bulk Avg</th>\n",
       "      <th>Fwd Packet/Bulk Avg</th>\n",
       "      <th>Fwd Bulk Rate Avg</th>\n",
       "      <th>Fwd Act Data Pkts</th>\n",
       "      <th>...</th>\n",
       "      <th>Packet Length Max</th>\n",
       "      <th>Packet Length Mean</th>\n",
       "      <th>Packet Length Std</th>\n",
       "      <th>Packet Length Variance</th>\n",
       "      <th>RST Flag Count</th>\n",
       "      <th>Average Packet Size</th>\n",
       "      <th>Fwd Segment Size Avg</th>\n",
       "      <th>Bwd Segment Size Avg</th>\n",
       "      <th>Subflow Fwd Bytes</th>\n",
       "      <th>Subflow Bwd Bytes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018674</td>\n",
       "      <td>0.027211</td>\n",
       "      <td>0.004687</td>\n",
       "      <td>0.215947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041436</td>\n",
       "      <td>0.125636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125636</td>\n",
       "      <td>0.457317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.457317</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.001147</td>\n",
       "      <td>0.00059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042272</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019890</td>\n",
       "      <td>0.018598</td>\n",
       "      <td>0.010679</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018598</td>\n",
       "      <td>0.045732</td>\n",
       "      <td>0.012796</td>\n",
       "      <td>0.028963</td>\n",
       "      <td>0.010059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.00031</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.145310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.355482</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019890</td>\n",
       "      <td>0.013767</td>\n",
       "      <td>0.010207</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013767</td>\n",
       "      <td>0.098448</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Flow Packets/s  Fwd PSH Flags  Fwd Packets/s  Bwd Packets/s  \\\n",
       "0        0.000340       0.000000       0.000681        0.00000   \n",
       "1        0.000082       0.000000       0.000164        0.00000   \n",
       "2        0.000015       0.000000       0.000030        0.00000   \n",
       "3        0.000879       0.128205       0.001147        0.00059   \n",
       "4        0.000319       0.923077       0.000316        0.00031   \n",
       "\n",
       "   FIN Flag Count  PSH Flag Count  Fwd Bytes/Bulk Avg  Fwd Packet/Bulk Avg  \\\n",
       "0             0.0        0.000000            0.000000             0.000000   \n",
       "1             0.0        0.000000            0.018674             0.027211   \n",
       "2             0.0        0.000000            0.000000             0.000000   \n",
       "3             0.0        0.042272            0.000000             0.000000   \n",
       "4             0.0        0.145310            0.000000             0.000000   \n",
       "\n",
       "   Fwd Bulk Rate Avg  Fwd Act Data Pkts  ...  Packet Length Max  \\\n",
       "0           0.000000           0.000000  ...           0.000000   \n",
       "1           0.004687           0.215947  ...           0.041436   \n",
       "2           0.000000           0.000000  ...           0.000000   \n",
       "3           0.000000           0.046512  ...           0.019890   \n",
       "4           0.000000           0.355482  ...           0.019890   \n",
       "\n",
       "   Packet Length Mean  Packet Length Std  Packet Length Variance  \\\n",
       "0            0.000000           0.000000                0.000000   \n",
       "1            0.125636           0.000000                0.000000   \n",
       "2            0.000000           0.000000                0.000000   \n",
       "3            0.018598           0.010679                0.000114   \n",
       "4            0.013767           0.010207                0.000104   \n",
       "\n",
       "   RST Flag Count  Average Packet Size  Fwd Segment Size Avg  \\\n",
       "0             0.0             0.000000              0.000000   \n",
       "1             0.0             0.125636              0.457317   \n",
       "2             0.0             0.000000              0.000000   \n",
       "3             0.0             0.018598              0.045732   \n",
       "4             0.0             0.013767              0.098448   \n",
       "\n",
       "   Bwd Segment Size Avg  Subflow Fwd Bytes  Subflow Bwd Bytes  \n",
       "0              0.000000           0.000000           0.000000  \n",
       "1              0.000000           0.457317           0.000000  \n",
       "2              0.000000           0.000000           0.000000  \n",
       "3              0.012796           0.028963           0.010059  \n",
       "4              0.000208           0.048780           0.000000  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e9e7aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Flow Packets/s', 'Fwd PSH Flags', 'Fwd Packets/s', 'Bwd Packets/s',\n",
       "       'FIN Flag Count', 'PSH Flag Count', 'Fwd Bytes/Bulk Avg',\n",
       "       'Fwd Packet/Bulk Avg', 'Fwd Bulk Rate Avg', 'Fwd Act Data Pkts',\n",
       "       'Fwd Packet Length Max', 'Fwd Packet Length Mean',\n",
       "       'Fwd Packet Length Std', 'Bwd Packet Length Max',\n",
       "       'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Packet Length Max',\n",
       "       'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance',\n",
       "       'RST Flag Count', 'Average Packet Size', 'Fwd Segment Size Avg',\n",
       "       'Bwd Segment Size Avg', 'Subflow Fwd Bytes', 'Subflow Bwd Bytes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "369846bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Flow Packets/s', 'Fwd PSH Flags', 'Fwd Packets/s', 'Bwd Packets/s',\n",
       "       'FIN Flag Count', 'PSH Flag Count', 'Fwd Bytes/Bulk Avg',\n",
       "       'Fwd Packet/Bulk Avg', 'Fwd Bulk Rate Avg', 'Fwd Act Data Pkts',\n",
       "       'Fwd Packet Length Max', 'Fwd Packet Length Mean',\n",
       "       'Fwd Packet Length Std', 'Bwd Packet Length Max',\n",
       "       'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Packet Length Max',\n",
       "       'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance',\n",
       "       'RST Flag Count', 'Average Packet Size', 'Fwd Segment Size Avg',\n",
       "       'Bwd Segment Size Avg', 'Subflow Fwd Bytes', 'Subflow Bwd Bytes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a021efe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(757027, 26)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfaf210d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hulk-Evasive    756987\n",
       "BENIGN              40\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attack.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13984993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the AnomalyExplanationUsingSHAP class with the specified number of anomalies\n",
    "exp_model = AnomalyExplanationUsingSHAP(num_anomalies=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "408a4bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data features: 26\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 26)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                270       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                352       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                352       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 26)                858       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,162\n",
      "Trainable params: 2,162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "4278/4278 - 6s - loss: 0.0028 - mse: 0.0027 - val_loss: 1.9803e-04 - val_mse: 1.3757e-04 - 6s/epoch - 1ms/step\n",
      "Epoch 2/5\n",
      "4278/4278 - 5s - loss: 1.7106e-04 - mse: 1.1474e-04 - val_loss: 1.4163e-04 - val_mse: 8.9240e-05 - 5s/epoch - 1ms/step\n",
      "Epoch 3/5\n",
      "4278/4278 - 5s - loss: 1.3467e-04 - mse: 8.4967e-05 - val_loss: 1.0404e-04 - val_mse: 5.5764e-05 - 5s/epoch - 1ms/step\n",
      "Epoch 4/5\n",
      "4278/4278 - 5s - loss: 9.5378e-05 - mse: 4.8852e-05 - val_loss: 8.0014e-05 - val_mse: 3.5788e-05 - 5s/epoch - 1ms/step\n",
      "Epoch 5/5\n",
      "4278/4278 - 5s - loss: 8.2983e-05 - mse: 4.0558e-05 - val_loss: 6.9714e-05 - val_mse: 2.8890e-05 - 5s/epoch - 1ms/step\n",
      "23658/23658 [==============================] - 23s 954us/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "13125/13125 [==============================] - 13s 1ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "13125/13125 [==============================] - 13s 1ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "13125/13125 [==============================] - 13s 1ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "13125/13125 [==============================] - 14s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Obtain explanations for anomaly detection using the instance of exp_model.\n",
    "# The explanations are generated for the given training data (X_train) and\n",
    "# the data to be explained (X_test). SHAP values will be returned for each explanation.\n",
    "all_feature_sets_explanation = exp_model.explain_unsupervised_data(train_data=X_train, \n",
    "                                                                   explain_data=X_test,\n",
    "                                                                   return_shap_values=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea1187c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{573735: [('Bwd Packet Length Std', -1),\n",
       "  ('Flow Packets/s', 0.46903039468179697),\n",
       "  ('Bwd Packets/s', 0.23831491110150982),\n",
       "  ('Fwd Packets/s', 0.10790319080536703),\n",
       "  ('RST Flag Count', 0.05254859136531481)],\n",
       " 4: [('Bwd Packet Length Std', -1),\n",
       "  ('Flow Packets/s', 0.45697032964607),\n",
       "  ('Bwd Packets/s', 0.22505317459077911),\n",
       "  ('Fwd Packets/s', 0.09795374685566322),\n",
       "  ('RST Flag Count', 0.05780982698058157)]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A dictionary containing explanations for each record in the dataset.(For training data)\n",
    "# The keys represent record indexes, and the values are lists of explanation feature sets.\n",
    "all_feature_sets_explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6621faff",
   "metadata": {},
   "outputs": [],
   "source": [
    " # define X_train\n",
    "X_train = benign_norm.to_numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d433068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Flow Packets/s', 'Fwd PSH Flags', 'Fwd Packets/s', 'Bwd Packets/s',\n",
       "       'FIN Flag Count', 'PSH Flag Count', 'Fwd Bytes/Bulk Avg',\n",
       "       'Fwd Packet/Bulk Avg', 'Fwd Bulk Rate Avg', 'Fwd Act Data Pkts',\n",
       "       'Fwd Packet Length Max', 'Fwd Packet Length Mean',\n",
       "       'Fwd Packet Length Std', 'Bwd Packet Length Max',\n",
       "       'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Packet Length Max',\n",
       "       'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance',\n",
       "       'RST Flag Count', 'Average Packet Size', 'Fwd Segment Size Avg',\n",
       "       'Bwd Segment Size Avg', 'Subflow Fwd Bytes', 'Subflow Bwd Bytes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e9a629c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train.columns))\n",
    "print(len(X_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21db8702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Flow Packets/s', 'Fwd PSH Flags', 'Fwd Packets/s', 'Bwd Packets/s',\n",
       "       'FIN Flag Count', 'PSH Flag Count', 'Fwd Bytes/Bulk Avg',\n",
       "       'Fwd Packet/Bulk Avg', 'Fwd Bulk Rate Avg', 'Fwd Act Data Pkts',\n",
       "       'Fwd Packet Length Max', 'Fwd Packet Length Mean',\n",
       "       'Fwd Packet Length Std', 'Bwd Packet Length Max',\n",
       "       'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Packet Length Max',\n",
       "       'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance',\n",
       "       'RST Flag Count', 'Average Packet Size', 'Fwd Segment Size Avg',\n",
       "       'Bwd Segment Size Avg', 'Subflow Fwd Bytes', 'Subflow Bwd Bytes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "144acbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9506/9506 [==============================] - 8s 867us/step\n"
     ]
    }
   ],
   "source": [
    "# threshold estimation using benign dataset\n",
    "# predictions for benign dataset\n",
    "import tensorflow\n",
    "autoencoder = tensorflow.keras.models.load_model(\"autoencoder.h5\")\n",
    "x_pred_benign = autoencoder.predict(X_train)\n",
    "benignMAE = np.mean(np.abs(X_train - x_pred_benign), axis=1)  # raw wise MAE calculation\n",
    "benign_norm['MAE'] = benignMAE  # add MAE as a column to benignMAE dataframe\n",
    "threshold = benign_norm['MAE'].quantile(0.99)  # 0.99 quantile threshold\n",
    "threshold_max = benign_norm['MAE'].max()  # maximum threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8777364b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23658/23658 [==============================] - 21s 880us/step\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "# autoencoder prediction results for attack dataset\n",
    "x_pred_attack = autoencoder.predict(X_test)\n",
    "attackMAE = np.mean(np.abs(X_test - x_pred_attack), axis=1)  # raw wise MAE calculation\n",
    "Attack['MAE'] = attackMAE\n",
    "Attack['pred_label'] = np.where(Attack['MAE']>threshold,1,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23379c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "26\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train.columns))\n",
    "print(len(X_test.columns))\n",
    "X_train = X_train.drop('MAE',1)\n",
    "print(len(X_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e28c8399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(757000, 27)\n"
     ]
    }
   ],
   "source": [
    "# x test with predicted labels\n",
    "X_test['pred_label'] = Attack['pred_label']\n",
    "\n",
    "# select only attack data \n",
    "X_test_attack = X_test[X_test['pred_label']==1]\n",
    "print(X_test_attack.shape)\n",
    "# drop pred_lable column \n",
    "X_test_attack_new = X_test_attack.drop(['pred_label'], axis=1, inplace=True)\n",
    "# print(X_test_attack_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ba70878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Flow Packets/s', 'Fwd PSH Flags', 'Fwd Packets/s', 'Bwd Packets/s',\n",
       "       'FIN Flag Count', 'PSH Flag Count', 'Fwd Bytes/Bulk Avg',\n",
       "       'Fwd Packet/Bulk Avg', 'Fwd Bulk Rate Avg', 'Fwd Act Data Pkts',\n",
       "       'Fwd Packet Length Max', 'Fwd Packet Length Mean',\n",
       "       'Fwd Packet Length Std', 'Bwd Packet Length Max',\n",
       "       'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Packet Length Max',\n",
       "       'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance',\n",
       "       'RST Flag Count', 'Average Packet Size', 'Fwd Segment Size Avg',\n",
       "       'Bwd Segment Size Avg', 'Subflow Fwd Bytes', 'Subflow Bwd Bytes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_attack.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5893ebfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45707300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03709b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test_attack.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3518271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_model = AnomalyExplanationUsingSHAP(num_anomalies=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dbfa9c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data features: 26\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 26)]              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                270       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 32)                352       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 32)                352       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 26)                858       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,162\n",
      "Trainable params: 2,162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "4278/4278 - 6s - loss: 0.0030 - mse: 0.0030 - val_loss: 1.6666e-04 - val_mse: 1.0979e-04 - 6s/epoch - 1ms/step\n",
      "Epoch 2/5\n",
      "4278/4278 - 5s - loss: 1.1130e-04 - mse: 5.5648e-05 - val_loss: 8.0316e-05 - val_mse: 2.7407e-05 - 5s/epoch - 1ms/step\n",
      "Epoch 3/5\n",
      "4278/4278 - 5s - loss: 7.6404e-05 - mse: 2.6926e-05 - val_loss: 6.3470e-05 - val_mse: 1.6447e-05 - 5s/epoch - 1ms/step\n",
      "Epoch 4/5\n",
      "4278/4278 - 5s - loss: 6.7469e-05 - mse: 2.2877e-05 - val_loss: 5.5463e-05 - val_mse: 1.2719e-05 - 5s/epoch - 1ms/step\n",
      "Epoch 5/5\n",
      "4278/4278 - 5s - loss: 6.2291e-05 - mse: 2.1098e-05 - val_loss: 5.2878e-05 - val_mse: 1.3000e-05 - 5s/epoch - 1ms/step\n",
      "23657/23657 [==============================] - 22s 929us/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "13125/13125 [==============================] - 14s 1ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "13125/13125 [==============================] - 13s 1ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "13125/13125 [==============================] - 13s 1ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "13125/13125 [==============================] - 13s 964us/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "13125/13125 [==============================] - 13s 966us/step\n"
     ]
    }
   ],
   "source": [
    "# model only for attack data 'X_test_attack'\n",
    "\n",
    "all_sets_explaining_features = exp_model.explain_unsupervised_data(train_data=X_train, \n",
    "                                                                   explain_data=X_test_attack,\n",
    "                                                                   return_shap_values=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d08242ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{602902: [('Fwd Packets/s', -1),\n",
       "  ('Bwd Packets/s', 0.22236648530019965),\n",
       "  ('Flow Packets/s', 0.20173666879070629)],\n",
       " 718029: [('Fwd Packets/s', -1),\n",
       "  ('Bwd Packets/s', 0.22101828334499202),\n",
       "  ('Flow Packets/s', 0.19778217858526576)],\n",
       " 384574: [('Fwd Packets/s', -1),\n",
       "  ('Bwd Packets/s', 0.22109119616268658),\n",
       "  ('Flow Packets/s', 0.20720393419003674)],\n",
       " 686625: [('Fwd Packets/s', -1),\n",
       "  ('Bwd Packets/s', 0.22355549765055377),\n",
       "  ('Flow Packets/s', 0.19770059419310443)],\n",
       " 573735: [('Fwd Packets/s', -1),\n",
       "  ('Bwd Packets/s', 0.1995736442579592),\n",
       "  ('Flow Packets/s', 0.17054043903637822)]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A dictionary containing explanations for each record in the dataset.(For Attack data)\n",
    "all_sets_explaining_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a308760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "op = dict(all_sets_explaining_features[573735])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7169c502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type (op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "df563b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convert(a):\n",
    "    it = iter(a)\n",
    "    res_dct = dict(zip(it, it))\n",
    "    return res_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4bb75619",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Convert(all_sets_explaining_features[573735])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "092f26b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "D=op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ba5d272f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x2b0ba8a5eb0>,\n",
       "  <matplotlib.axis.XTick at 0x2b0ee731880>,\n",
       "  <matplotlib.axis.XTick at 0x2b0c975ac70>],\n",
       " [Text(0, 0, 'Fwd Packets/s'),\n",
       "  Text(1, 0, 'Bwd Packets/s'),\n",
       "  Text(2, 0, 'Flow Packets/s')])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(range(len(D)), list(D.values()), align='center')\n",
    "plt.xticks(rotation = 55) \n",
    "plt.xticks(range(len(D)), list(D.keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
